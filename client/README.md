# datamaster


Uploading
---------

todo - 
handle many remotes in the same way that you handle many branches - store the active one in a database.

configing settings - you need a default object that will detail where things go. when someone inits a new
workspace, copy those. or ask questions to config it.

test the fallback mechanisms.
- works at root?
- fallback to local?
- no local


need to make a setup.py file and list our dm util

change list util:
- dm list should list the existing datasets and number of files.
- dm list name should list the files/projects in that dataset. have a recursive command

carl - writes a lot of code
jordan! - would be honest
niki - challenge to explain but very critical
kenya

get danielle to schedule with keith and the other woman.

our system really 


for meetings:
- what tech do they use for their data pipelines?


our system works fine for python workflows. how common are those? how would we work with things like direct S3 to S3 workloads?
what engines like airflow do we support and how?
what do we do with pyspark?

*** - maybe we should keep a "filesystemtype" stamp that tracks differently? ***

handle that red line in vscode

how important is it that we have this python easy demo?

log to server:
creating file
relationships
